{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf77e1c-86d7-4250-a691-17939e3bcfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n╔═══════════════════════════════════════════════════════════════╗\n║                                                               ║\n║                  ✅ DATA QUALITY CHECKS                       ║\n║                                                               ║\n║        Validating data quality across silver and gold layers  ║\n║                                                               ║\n╚═══════════════════════════════════════════════════════════════╝\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                                                               ║\n",
    "║                  ✅ DATA QUALITY CHECKS                       ║\n",
    "║                                                               ║\n",
    "║        Validating data quality across silver and gold layers  ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23e46e6-cb21-4a87-bd01-133352c67fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n╔═══════════════════════════════════════════════════════════════╗\n║                                                               ║\n║                  ✅ DATA QUALITY CHECKS                       ║\n║                                                               ║\n║        Validating data quality across silver and gold layers  ║\n║                                                               ║\n╚═══════════════════════════════════════════════════════════════╝\n\n\nQuality Check Execution:\n  Run ID: 20260221_214901\n  Execution Date: 2026-02-21\n  Timestamp: 2026-02-21 21:49:01.501534\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                                                               ║\n",
    "║                  ✅ DATA QUALITY CHECKS                       ║\n",
    "║                                                               ║\n",
    "║        Validating data quality across silver and gold layers  ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# STEP 1: Generate Execution ID (no parameters needed!)\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "execution_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Quality Check Execution:\n",
    "  Run ID: {run_id}\n",
    "  Execution Date: {execution_date}\n",
    "  Timestamp: {datetime.now()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f356ec9d-6b81-49e5-b417-690a893d243f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDatabase Configuration:\n  Catalog: fintech_analytics\n  Silver Schema: silver\n  Gold Schema: gold\n  Audit Schema: audit\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Set Configuration\n",
    "\n",
    "catalog = \"fintech_analytics\"\n",
    "silver_schema = \"silver\"\n",
    "gold_schema = \"gold\"\n",
    "audit_schema = \"audit\"\n",
    "\n",
    "print(f\"\"\"\n",
    "Database Configuration:\n",
    "  Catalog: {catalog}\n",
    "  Silver Schema: {silver_schema}\n",
    "  Gold Schema: {gold_schema}\n",
    "  Audit Schema: {audit_schema}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b74d23-254f-4f08-9bf8-4be1f84423d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 1: SILVER FACT TABLE EXISTS\n================================================================================\n✅ Silver table exists: 985 rows\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Quality Check 1 - Silver Table Exists\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 1: SILVER FACT TABLE EXISTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    silver_table = f\"{catalog}.{silver_schema}.silver_fact_insider_transactions\"\n",
    "    silver_count = spark.sql(f\"SELECT COUNT(*) as count FROM {silver_table}\").collect()[0]['count']\n",
    "    \n",
    "    check_1_status = \"PASS\"\n",
    "    check_1_message = f\"Silver table exists: {silver_count:,} rows\"\n",
    "    print(f\"✅ {check_1_message}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    check_1_status = \"FAIL\"\n",
    "    check_1_message = f\"Error: {str(e)[:100]}\"\n",
    "    print(f\"❌ {check_1_message}\")\n",
    "    silver_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bec5fe1-0d46-47ae-8710-2f548579ad7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 2: REQUIRED COLUMNS IN SILVER\n================================================================================\n✅ All 12 required columns present\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Quality Check 2 - Required Columns\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 2: REQUIRED COLUMNS IN SILVER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required_columns = [\n",
    "    \"transaction_id\",\n",
    "    \"company_name\",\n",
    "    \"filing_date\",\n",
    "    \"insider_name\",\n",
    "    \"transaction_amount\",\n",
    "    \"shares\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    silver_table = f\"{catalog}.{silver_schema}.silver_fact_insider_transactions\"\n",
    "    df = spark.sql(f\"SELECT * FROM {silver_table} LIMIT 1\")\n",
    "    actual_columns = set(df.columns)\n",
    "    # STEP 4: Quality Check 2 - Required Columns\n",
    "\n",
    "    required_columns = [\n",
    "    \"cik\",\n",
    "    \"company_name\",\n",
    "    \"filing_date\",\n",
    "    \"insider_name\",\n",
    "    \"security_title\",\n",
    "    \"transaction_date\",\n",
    "    \"shares\",\n",
    "    \"price_per_share\",\n",
    "    \"transaction_code\",\n",
    "    \"acquired_disposed\",\n",
    "    \"shares_after_transaction\",\n",
    "    \"confidence_score\"\n",
    "    ]\n",
    "\n",
    "    required_set = set(required_columns)\n",
    "    missing = required_set - actual_columns\n",
    "    extra = actual_columns - required_set\n",
    "    \n",
    "    if not missing:\n",
    "        check_2_status = \"PASS\"\n",
    "        check_2_message = f\"All {len(required_columns)} required columns present\"\n",
    "        print(f\"✅ {check_2_message}\")\n",
    "    else:\n",
    "        check_2_status = \"FAIL\"\n",
    "        check_2_message = f\"Missing: {', '.join(missing)}\"\n",
    "        print(f\"❌ {check_2_message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    check_2_status = \"FAIL\"\n",
    "    check_2_message = f\"Error: {str(e)[:100]}\"\n",
    "    print(f\"❌ {check_2_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bc72b3-cc8b-4d7a-bb87-e09519190aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 3: ROW COUNT VALIDATION\n================================================================================\n✅ Row count 985 is valid (minimum: 1)\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Quality Check 3 - Row Count\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 3: ROW COUNT VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    silver_table = f\"{catalog}.{silver_schema}.silver_fact_insider_transactions\"\n",
    "    count_result = spark.sql(f\"SELECT COUNT(*) as cnt FROM {silver_table}\").collect()[0]['cnt']\n",
    "    min_threshold = 1  # Change to your minimum\n",
    "    \n",
    "    if count_result >= min_threshold:\n",
    "        check_3_status = \"PASS\"\n",
    "        check_3_message = f\"Row count {count_result:,} is valid (minimum: {min_threshold})\"\n",
    "        print(f\"✅ {check_3_message}\")\n",
    "    else:\n",
    "        check_3_status = \"FAIL\"\n",
    "        check_3_message = f\"Row count {count_result:,} below minimum {min_threshold}\"\n",
    "        print(f\"❌ {check_3_message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    check_3_status = \"FAIL\"\n",
    "    check_3_message = f\"Error: {str(e)[:100]}\"\n",
    "    print(f\"❌ {check_3_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019ea364-8100-4c62-9ad6-26800e1d5166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 4: PRIMARY KEY NOT NULL\n================================================================================\n✅ No null values in transaction_id (primary key)\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Quality Check 4 - No Null IDs\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 4: PRIMARY KEY NOT NULL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    silver_table = f\"{catalog}.{silver_schema}.silver_fact_insider_transactions\"\n",
    "    null_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as null_count\n",
    "    FROM {silver_table}\n",
    "    WHERE cik IS NULL and price_per_share IS NULL\n",
    "    \"\"\").collect()[0]['null_count']\n",
    "    \n",
    "    if null_count == 0:\n",
    "        check_4_status = \"PASS\"\n",
    "        check_4_message = \"No null values in transaction_id (primary key)\"\n",
    "        print(f\"✅ {check_4_message}\")\n",
    "    else:\n",
    "        check_4_status = \"FAIL\"\n",
    "        check_4_message = f\"Found {null_count:,} null values in transaction_id\"\n",
    "        print(f\"❌ {check_4_message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    check_4_status = \"FAIL\"\n",
    "    check_4_message = f\"Error: {str(e)[:100]}\"\n",
    "    print(f\"❌ {check_4_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae02317-f6c1-4fe1-889f-be9dda349462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 5: GOLD TABLE EXISTS\n================================================================================\n✅ Gold table Gold insider summary by company: 33 rows\n✅ Gold table Gold KPI summary: 6 rows\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Quality Check 5 - Gold Table Exists\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 5: GOLD TABLE EXISTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    gold_table = f\"{catalog}.{gold_schema}.gold_insider_summary_by_company\"\n",
    "    gold_count = spark.sql(f\"SELECT COUNT(*) as count FROM {gold_table}\").collect()[0]['count']\n",
    "    \n",
    "    check_5_status = \"PASS\"\n",
    "    check_5_message = f\"Gold table Gold insider summary by company: {gold_count:,} rows\"\n",
    "    print(f\"✅ {check_5_message}\")\n",
    "    gold_table = f\"{catalog}.{gold_schema}.gold_kpi_summary\"\n",
    "    gold_count = spark.sql(f\"SELECT COUNT(*) as count FROM {gold_table}\").collect()[0]['count']\n",
    "    \n",
    "    check_5_status = \"PASS\"\n",
    "    check_5_message = f\"Gold table Gold KPI summary: {gold_count:,} rows\"\n",
    "    print(f\"✅ {check_5_message}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    check_5_status = \"FAIL\"\n",
    "    check_5_message = f\"Error: {str(e)[:100]}\"\n",
    "    print(f\"❌ {check_5_message}\")\n",
    "    gold_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0128fd7e-fa68-4c50-ac78-0512fd9e9aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nCHECK 6: DATA CONSISTENCY (SILVER → GOLD)\n================================================================================\n✅ Consistency OK: 28 companies aligned across Silver and Gold\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 8: Quality Check 6 - Data Consistency (Silver vs Gold)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 6: DATA CONSISTENCY (SILVER → GOLD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    silver_table = f\"{catalog}.{silver_schema}.silver_fact_insider_transactions\"\n",
    "    gold_table = f\"{catalog}.{gold_schema}.gold_insider_summary_by_company\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Count DISTINCT companies in both layers\n",
    "    # --------------------------------------------------------\n",
    "    silver_companies = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT company_name) AS cnt\n",
    "        FROM {silver_table}\n",
    "    \"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "    gold_companies = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT company_name) AS cnt\n",
    "        FROM {gold_table}\n",
    "    \"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Lineage Validation (Gold must come from Silver)\n",
    "    # --------------------------------------------------------\n",
    "    mismatch_df = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT company_name\n",
    "        FROM {gold_table}\n",
    "\n",
    "        EXCEPT\n",
    "\n",
    "        SELECT DISTINCT company_name\n",
    "        FROM {silver_table}\n",
    "    \"\"\")\n",
    "\n",
    "    mismatch_count = mismatch_df.count()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Final Validation Logic\n",
    "    # --------------------------------------------------------\n",
    "    if mismatch_count == 0 and silver_companies == gold_companies:\n",
    "        check_6_status = \"PASS\"\n",
    "        check_6_message = (\n",
    "            f\"Consistency OK: {silver_companies} companies aligned across Silver and Gold\"\n",
    "        )\n",
    "        print(f\"✅ {check_6_message}\")\n",
    "\n",
    "    else:\n",
    "        check_6_status = \"FAIL\"\n",
    "        check_6_message = (\n",
    "            f\"Mismatch detected → Silver: {silver_companies}, Gold: {gold_companies}, \"\n",
    "            f\"Extra in Gold: {mismatch_count}\"\n",
    "        )\n",
    "        print(f\"❌ {check_6_message}\")\n",
    "\n",
    "        print(\"\\n⚠️ Companies present in GOLD but missing in SILVER:\")\n",
    "        mismatch_df.show(truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    check_6_status = \"FAIL\"\n",
    "    check_6_message = f\"Error during consistency check: {str(e)[:120]}\"\n",
    "    print(f\"❌ {check_6_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57788f99-3b22-4bb9-bd6d-d7a28922fa03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nLOGGING RESULTS TO AUDIT TABLE\n================================================================================\n✅ Logged 6 checks to audit table\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Attempt to Log to Audit Table (Safe - won't crash if fails)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOGGING RESULTS TO AUDIT TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "quality_checks = [\n",
    "    {\"check_name\": \"silver_table_exists\", \"status\": check_1_status, \"message\": check_1_message},\n",
    "    {\"check_name\": \"required_columns\", \"status\": check_2_status, \"message\": check_2_message},\n",
    "    {\"check_name\": \"row_count\", \"status\": check_3_status, \"message\": check_3_message},\n",
    "    {\"check_name\": \"no_null_ids\", \"status\": check_4_status, \"message\": check_4_message},\n",
    "    {\"check_name\": \"gold_table_exists\", \"status\": check_5_status, \"message\": check_5_message},\n",
    "    {\"check_name\": \"data_consistency\", \"status\": check_6_status, \"message\": check_6_message},\n",
    "]\n",
    "\n",
    "logged_count = 0\n",
    "for check in quality_checks:\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {catalog}.{audit_schema}.data_quality_checks\n",
    "        (run_id, table_name, check_name, status, message, execution_date, created_at)\n",
    "        VALUES (\n",
    "            '{run_id}',\n",
    "            'silver_fact_insider_transactions',\n",
    "            '{check[\"check_name\"]}',\n",
    "            '{check[\"status\"]}',\n",
    "            '{check[\"message\"].replace(\"'\", \"''\")[:200]}',\n",
    "            CAST('{execution_date}' AS DATE),\n",
    "            CURRENT_TIMESTAMP()\n",
    "        )\n",
    "        \"\"\")\n",
    "        logged_count += 1\n",
    "    except Exception as e:\n",
    "        # Silently continue if audit table doesn't exist\n",
    "        pass\n",
    "\n",
    "if logged_count > 0:\n",
    "    print(f\"✅ Logged {logged_count} checks to audit table\")\n",
    "else:\n",
    "    print(f\"⚠️  Could not log to audit table (table may not exist)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5134895-eae6-4a97-8bfc-a7a5c0322fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDCCA QUALITY CHECK SUMMARY\n================================================================================\n\nResults:\n  Total Checks: 6\n  Passed: 6 ✅\n  Failed: 0 ❌\n  Success Rate: 100.0%\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Failed</th><th>Passed</th><th>Success %</th><th>Total</th></tr></thead><tbody><tr><td>0</td><td>6</td><td>100.0%</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         6,
         "100.0%",
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Failed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Passed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Success %",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 10: Summary Report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\uD83D\uDCCA QUALITY CHECK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = len(quality_checks)\n",
    "passed = sum(1 for c in quality_checks if c[\"status\"] == \"PASS\")\n",
    "failed = sum(1 for c in quality_checks if c[\"status\"] == \"FAIL\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Results:\n",
    "  Total Checks: {total}\n",
    "  Passed: {passed} ✅\n",
    "  Failed: {failed} ❌\n",
    "  Success Rate: {100 * passed / total:.1f}%\n",
    "\"\"\")\n",
    "\n",
    "# Display as table\n",
    "summary_df = spark.createDataFrame([\n",
    "    {\"Total\": total, \"Passed\": passed, \"Failed\": failed, \"Success %\": f\"{100 * passed / total:.1f}%\"}\n",
    "])\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453e273d-84ae-4602-b925-94a915dbc151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDetailed Results:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>check_name</th><th>message</th><th>status</th></tr></thead><tbody><tr><td>silver_table_exists</td><td>Silver table exists: 985 rows</td><td>PASS</td></tr><tr><td>required_columns</td><td>All 12 required columns present</td><td>PASS</td></tr><tr><td>row_count</td><td>Row count 985 is valid (minimum: 1)</td><td>PASS</td></tr><tr><td>no_null_ids</td><td>No null values in transaction_id (primary key)</td><td>PASS</td></tr><tr><td>gold_table_exists</td><td>Gold table Gold KPI summary: 6 rows</td><td>PASS</td></tr><tr><td>data_consistency</td><td>Consistency OK: 28 companies aligned across Silver and Gold</td><td>PASS</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "silver_table_exists",
         "Silver table exists: 985 rows",
         "PASS"
        ],
        [
         "required_columns",
         "All 12 required columns present",
         "PASS"
        ],
        [
         "row_count",
         "Row count 985 is valid (minimum: 1)",
         "PASS"
        ],
        [
         "no_null_ids",
         "No null values in transaction_id (primary key)",
         "PASS"
        ],
        [
         "gold_table_exists",
         "Gold table Gold KPI summary: 6 rows",
         "PASS"
        ],
        [
         "data_consistency",
         "Consistency OK: 28 companies aligned across Silver and Gold",
         "PASS"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "check_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFinal Status: SUCCESS\nQuality checks complete!\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 11: Detailed Results Table\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "\n",
    "results_df = spark.createDataFrame(quality_checks)\n",
    "display(results_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "# STEP 12: Safe Return Status\n",
    "\n",
    "all_passed = all(c[\"status\"] == \"PASS\" for c in quality_checks)\n",
    "\n",
    "result = {\n",
    "    \"status\": \"SUCCESS\" if all_passed else \"PARTIAL\",\n",
    "    \"total_checks\": total,\n",
    "    \"passed\": passed,\n",
    "    \"failed\": failed\n",
    "}\n",
    "\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(result)\n",
    "except:\n",
    "    # OK if this fails (not in a job)\n",
    "    pass\n",
    "\n",
    "print(f\"\"\"\n",
    "Final Status: {result['status']}\n",
    "Quality checks complete!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920e8e10-0354-4f9a-8f92-0d0a59eb1ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ ALL QUALITY CHECKS PASSED!\n\nEverything is good:\n  ✓ Silver layer has data\n  ✓ Gold layer has data\n  ✓ Data is consistent\n  ✓ No critical nulls\n  ✓ All columns present\n  \nPipeline can continue to optimization!\n    \n✅ Quality Checks Notebook Complete\n"
     ]
    }
   ],
   "source": [
    "# STEP 13: Next Steps\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\"\"\n",
    "✅ ALL QUALITY CHECKS PASSED!\n",
    "\n",
    "Everything is good:\n",
    "  ✓ Silver layer has data\n",
    "  ✓ Gold layer has data\n",
    "  ✓ Data is consistent\n",
    "  ✓ No critical nulls\n",
    "  ✓ All columns present\n",
    "  \n",
    "Pipeline can continue to optimization!\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "⚠️ SOME QUALITY CHECKS FAILED\n",
    "\n",
    "Review the checks above and debug:\n",
    "  \n",
    "For silver table issues:\n",
    "  SELECT * FROM fintech_analytics.silver.silver_fact_insider_transactions LIMIT 10;\n",
    "  \n",
    "For gold table issues:\n",
    "  SELECT * FROM fintech_analytics.gold.gold_company_summary LIMIT 10;\n",
    "  \n",
    "For consistency issues:\n",
    "  SELECT company_name, COUNT(*) as count\n",
    "  FROM fintech_analytics.silver.silver_fact_insider_transactions\n",
    "  GROUP BY company_name;\n",
    "    \"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "print(\"✅ Quality Checks Notebook Complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_data_quality_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}