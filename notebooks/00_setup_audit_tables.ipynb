{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163b1ef9-957a-4669-9ae2-26f75553ad63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n╔═══════════════════════════════════════════════════════════════╗\n║                                                               ║\n║        \uD83D\uDD27 PHASE 4 AUDIT INFRASTRUCTURE INITIALIZATION         ║\n║                                                               ║\n║        This will create 6 audit tables in the audit schema    ║\n║        for tracking pipeline execution, errors, and quality   ║\n║                                                               ║\n╚═══════════════════════════════════════════════════════════════╝\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                                                               ║\n",
    "║        \uD83D\uDD27 PHASE 4 AUDIT INFRASTRUCTURE INITIALIZATION         ║\n",
    "║                                                               ║\n",
    "║        This will create 6 audit tables in the audit schema    ║\n",
    "║        for tracking pipeline execution, errors, and quality   ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfc656b-e8d6-4fbc-a42a-e7e47fd09ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n  Catalog: fintech_analytics\n  Schema: audit\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Set Configuration\n",
    "\n",
    "catalog_name = \"fintech_analytics\"\n",
    "schema_name = \"audit\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Catalog: {catalog_name}\")\n",
    "print(f\"  Schema: {schema_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e72a6cb-9edc-40fc-873b-c97ef5c5b829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema created: fintech_analytics.audit\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Create Schema\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "print(f\"✅ Schema created: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95fa2e2-736d-43f1-8ad8-481e2f215e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 1 Created: pipeline_runs\n   Purpose: High-level tracking of each pipeline execution\n   Columns: run_id, execution_date, status, start_time, duration, etc.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Create TABLE 1 - Pipeline Runs (high-level tracking)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.pipeline_runs (\n",
    "    run_id STRING NOT NULL,\n",
    "    execution_date DATE NOT NULL,\n",
    "    environment STRING,\n",
    "    run_mode STRING,\n",
    "    total_tasks INT,\n",
    "    successful_tasks INT,\n",
    "    failed_tasks INT,\n",
    "    skipped_tasks INT,\n",
    "    status STRING,\n",
    "    start_time TIMESTAMP,\n",
    "    end_time TIMESTAMP,\n",
    "    duration_seconds BIGINT,\n",
    "    error_summary STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (execution_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 1 Created: pipeline_runs\")\n",
    "print(\"   Purpose: High-level tracking of each pipeline execution\")\n",
    "print(\"   Columns: run_id, execution_date, status, start_time, duration, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e962d467-3bba-4723-89b3-09070dd4528e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 2 Created: task_runs\n   Purpose: Detailed tracking of each task (silver, gold, quality, optimize)\n   Columns: task_name, status, attempt, duration_seconds, error_message, stack_trace\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Create TABLE 2 - Task Runs (detailed task tracking)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.task_runs (\n",
    "    run_id STRING NOT NULL,\n",
    "    task_name STRING NOT NULL,\n",
    "    task_type STRING,\n",
    "    execution_order INT,\n",
    "    attempt INT,\n",
    "    status STRING,\n",
    "    start_time TIMESTAMP,\n",
    "    end_time TIMESTAMP,\n",
    "    duration_seconds BIGINT,\n",
    "    rows_processed BIGINT,\n",
    "    rows_inserted BIGINT,\n",
    "    rows_updated BIGINT,\n",
    "    error_message STRING,\n",
    "    error_type STRING,\n",
    "    stack_trace STRING,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (run_id)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 2 Created: task_runs\")\n",
    "print(\"   Purpose: Detailed tracking of each task (silver, gold, quality, optimize)\")\n",
    "print(\"   Columns: task_name, status, attempt, duration_seconds, error_message, stack_trace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f18be9-e1c8-4ba7-b3b3-5c2ab6990d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 3 Created: data_quality_checks\n   Purpose: Results of all data quality validations\n   Columns: table_name, check_name, status, expected_value, actual_value, message\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Create TABLE 3 - Data Quality Checks (quality validation results)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.data_quality_checks (\n",
    "    run_id STRING NOT NULL,\n",
    "    table_name STRING NOT NULL,\n",
    "    check_name STRING NOT NULL,\n",
    "    check_type STRING,\n",
    "    expected_value STRING,\n",
    "    actual_value STRING,\n",
    "    status STRING,\n",
    "    message STRING,\n",
    "    execution_date DATE,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (execution_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 3 Created: data_quality_checks\")\n",
    "print(\"   Purpose: Results of all data quality validations\")\n",
    "print(\"   Columns: table_name, check_name, status, expected_value, actual_value, message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ea59e5-bab7-4b0e-bae1-bf9326a6cfd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 4 Created: watermarks\n   Purpose: Track last processed date for incremental pipelines\n   Columns: source_table, target_table, last_processed_date, row_count\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Create TABLE 4 - Watermarks (incremental processing state)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.watermarks (\n",
    "    source_table STRING NOT NULL,\n",
    "    target_table STRING NOT NULL,\n",
    "    last_processed_date DATE,\n",
    "    last_processed_timestamp TIMESTAMP,\n",
    "    row_count BIGINT,\n",
    "    updated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 4 Created: watermarks\")\n",
    "print(\"   Purpose: Track last processed date for incremental pipelines\")\n",
    "print(\"   Columns: source_table, target_table, last_processed_date, row_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862881e6-ad62-450f-8723-17450b300de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 5 Created: error_log\n   Purpose: Complete error tracking with stack traces\n   Columns: error_id, error_type, error_message, full_traceback, severity, is_resolved\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Create TABLE 5 - Error Log (detailed error tracking)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.error_log (\n",
    "    error_id STRING NOT NULL,\n",
    "    run_id STRING,\n",
    "    task_name STRING,\n",
    "    error_type STRING,\n",
    "    error_message STRING,\n",
    "    full_traceback STRING,\n",
    "    context_data STRING,\n",
    "    severity STRING,\n",
    "    is_resolved BOOLEAN,\n",
    "    resolution_notes STRING,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 5 Created: error_log\")\n",
    "print(\"   Purpose: Complete error tracking with stack traces\")\n",
    "print(\"   Columns: error_id, error_type, error_message, full_traceback, severity, is_resolved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15575951-668e-416f-8f03-7ad9aa950194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TABLE 6 Created: repair_history\n   Purpose: Track recovery/repair attempts when pipelines fail\n   Columns: repair_id, original_run_id, repair_type, repair_status, rows_reprocessed\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Create TABLE 6 - Repair History (recovery tracking)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.repair_history (\n",
    "    repair_id STRING NOT NULL,\n",
    "    original_run_id STRING NOT NULL,\n",
    "    original_task_name STRING NOT NULL,\n",
    "    repair_type STRING,\n",
    "    repair_status STRING,\n",
    "    rows_reprocessed BIGINT,\n",
    "    attempted_at TIMESTAMP,\n",
    "    completed_at TIMESTAMP,\n",
    "    notes STRING,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ TABLE 6 Created: repair_history\")\n",
    "print(\"   Purpose: Track recovery/repair attempts when pipelines fail\")\n",
    "print(\"   Columns: repair_id, original_run_id, repair_type, repair_status, rows_reprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fade89e8-8778-4572-abde-79048eaca29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n✅ VERIFICATION: All Tables Successfully Created\n================================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>table_type</th><th>comment</th></tr></thead><tbody><tr><td>data_quality_checks</td><td>MANAGED</td><td>null</td></tr><tr><td>error_log</td><td>MANAGED</td><td>null</td></tr><tr><td>pipeline_runs</td><td>MANAGED</td><td>null</td></tr><tr><td>repair_history</td><td>MANAGED</td><td>null</td></tr><tr><td>task_runs</td><td>MANAGED</td><td>null</td></tr><tr><td>watermarks</td><td>MANAGED</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "data_quality_checks",
         "MANAGED",
         null
        ],
        [
         "error_log",
         "MANAGED",
         null
        ],
        [
         "pipeline_runs",
         "MANAGED",
         null
        ],
        [
         "repair_history",
         "MANAGED",
         null
        ],
        [
         "task_runs",
         "MANAGED",
         null
        ],
        [
         "watermarks",
         "MANAGED",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Name of the relation.\"}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"Type of the table such as 'MANAGED', 'FOREIGN', 'VIEW', etc.\"}",
         "name": "table_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"An optional comment that describes the relation.\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 9: Verify All Tables Created\n",
    "\n",
    "verification = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    table_name,\n",
    "    table_type,\n",
    "    comment\n",
    "FROM {catalog_name}.information_schema.tables\n",
    "WHERE table_schema = '{schema_name}'\n",
    "ORDER BY table_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ VERIFICATION: All Tables Successfully Created\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display(verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3900ddd4-f08f-4072-8ff3-f95d9ae9d462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCurrent Row Counts (should all be 0 initially):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>row_count</th></tr></thead><tbody><tr><td>pipeline_runs</td><td>0</td></tr><tr><td>task_runs</td><td>0</td></tr><tr><td>data_quality_checks</td><td>0</td></tr><tr><td>watermarks</td><td>0</td></tr><tr><td>error_log</td><td>0</td></tr><tr><td>repair_history</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "pipeline_runs",
         0
        ],
        [
         "task_runs",
         0
        ],
        [
         "data_quality_checks",
         0
        ],
        [
         "watermarks",
         0
        ],
        [
         "error_log",
         0
        ],
        [
         "repair_history",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "row_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 10: Check Table Row Counts\n",
    "\n",
    "row_counts = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    'pipeline_runs' as table_name, COUNT(*) as row_count FROM {catalog_name}.{schema_name}.pipeline_runs\n",
    "UNION ALL\n",
    "SELECT 'task_runs', COUNT(*) FROM {catalog_name}.{schema_name}.task_runs\n",
    "UNION ALL\n",
    "SELECT 'data_quality_checks', COUNT(*) FROM {catalog_name}.{schema_name}.data_quality_checks\n",
    "UNION ALL\n",
    "SELECT 'watermarks', COUNT(*) FROM {catalog_name}.{schema_name}.watermarks\n",
    "UNION ALL\n",
    "SELECT 'error_log', COUNT(*) FROM {catalog_name}.{schema_name}.error_log\n",
    "UNION ALL\n",
    "SELECT 'repair_history', COUNT(*) FROM {catalog_name}.{schema_name}.repair_history\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nCurrent Row Counts (should all be 0 initially):\")\n",
    "display(row_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebf3524-4f23-430d-999b-efc844efddbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample data inserted (for testing)\n"
     ]
    }
   ],
   "source": [
    "# STEP 11: Create Sample Data (for testing)\n",
    "\n",
    "# Sample pipeline run\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {catalog_name}.{schema_name}.pipeline_runs (\n",
    "    run_id, execution_date, environment, run_mode, total_tasks, \n",
    "    successful_tasks, failed_tasks, skipped_tasks, status, start_time, end_time\n",
    ")\n",
    "VALUES (\n",
    "    'TEST_20260220_000000',\n",
    "    CURRENT_DATE(),\n",
    "    'dev',\n",
    "    'full',\n",
    "    4,\n",
    "    4,\n",
    "    0,\n",
    "    0,\n",
    "    'SUCCESS',\n",
    "    CURRENT_TIMESTAMP(),\n",
    "    CURRENT_TIMESTAMP()\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Sample data inserted (for testing)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e47a11d-28b7-490b-a2cd-bbc5ec7db91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83C\uDF89 SETUP COMPLETE!\n================================================================================\n\nAudit Infrastructure Initialized Successfully!\n\nCREATED 6 TABLES:\n  1. pipeline_runs           - High-level pipeline execution tracking\n  2. task_runs              - Detailed task-level execution details\n  3. data_quality_checks    - Quality validation results\n  4. watermarks             - Incremental processing state\n  5. error_log              - Detailed error tracking\n  6. repair_history         - Recovery attempt tracking\n\nLOCATION:\n  Schema: fintech_analytics.audit\n\nNEXT STEPS:\n  1. Create silver notebook (Step 6)\n  2. Create gold notebook (Step 7)\n  3. Create quality checks notebook (Step 8)\n  4. Create optimization notebook (Step 9)\n  5. Create master orchestration notebook (Step 10)\n  6. Create Databricks Workflows job (Step 11)\n  7. Run first test!\n\nVERIFY SETUP:\n  Run this query anytime to check audit tables:\n  \n  SELECT * FROM fintech_analytics.audit.pipeline_runs\n  ORDER BY created_at DESC\n  LIMIT 10;\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 12: Final Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\uD83C\uDF89 SETUP COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "Audit Infrastructure Initialized Successfully!\n",
    "\n",
    "CREATED 6 TABLES:\n",
    "  1. pipeline_runs           - High-level pipeline execution tracking\n",
    "  2. task_runs              - Detailed task-level execution details\n",
    "  3. data_quality_checks    - Quality validation results\n",
    "  4. watermarks             - Incremental processing state\n",
    "  5. error_log              - Detailed error tracking\n",
    "  6. repair_history         - Recovery attempt tracking\n",
    "\n",
    "LOCATION:\n",
    "  Schema: {catalog_name}.{schema_name}\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Create silver notebook (Step 6)\n",
    "  2. Create gold notebook (Step 7)\n",
    "  3. Create quality checks notebook (Step 8)\n",
    "  4. Create optimization notebook (Step 9)\n",
    "  5. Create master orchestration notebook (Step 10)\n",
    "  6. Create Databricks Workflows job (Step 11)\n",
    "  7. Run first test!\n",
    "\n",
    "VERIFY SETUP:\n",
    "  Run this query anytime to check audit tables:\n",
    "  \n",
    "  SELECT * FROM {catalog_name}.{schema_name}.pipeline_runs\n",
    "  ORDER BY created_at DESC\n",
    "  LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ae099a-6230-47e6-bceb-85941ccc9ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from pipeline_runs table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_id</th><th>execution_date</th><th>environment</th><th>status</th><th>total_tasks</th><th>successful_tasks</th></tr></thead><tbody><tr><td>TEST_20260220_000000</td><td>2026-02-21</td><td>dev</td><td>SUCCESS</td><td>4</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "TEST_20260220_000000",
         "2026-02-21",
         "dev",
         "SUCCESS",
         4,
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "execution_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "environment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_tasks",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "successful_tasks",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ SETUP NOTEBOOK COMPLETE!\n\nNow proceed to:\n  STEP 6: Create & Adapt Silver Notebook\n  STEP 7: Create & Adapt Gold Notebook\n  STEP 8: Create Quality Checks Notebook\n  STEP 9: Create Table Optimization Notebook\n  STEP 10: Create Master Orchestration Notebook\n  STEP 11: Create Databricks Workflows Job\n\nRefer to DEPLOYMENT_STEP_BY_STEP.md for detailed instructions on each step.\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 13: Display Sample Data to Verify\n",
    "\n",
    "print(\"Sample data from pipeline_runs table:\")\n",
    "\n",
    "sample_data = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    run_id,\n",
    "    execution_date,\n",
    "    environment,\n",
    "    status,\n",
    "    total_tasks,\n",
    "    successful_tasks\n",
    "FROM {catalog_name}.{schema_name}.pipeline_runs\n",
    "ORDER BY created_at DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(sample_data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\"\"\n",
    "✅ SETUP NOTEBOOK COMPLETE!\n",
    "\n",
    "Now proceed to:\n",
    "  STEP 6: Create & Adapt Silver Notebook\n",
    "  STEP 7: Create & Adapt Gold Notebook\n",
    "  STEP 8: Create Quality Checks Notebook\n",
    "  STEP 9: Create Table Optimization Notebook\n",
    "  STEP 10: Create Master Orchestration Notebook\n",
    "  STEP 11: Create Databricks Workflows Job\n",
    "\n",
    "Refer to DEPLOYMENT_STEP_BY_STEP.md for detailed instructions on each step.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_SETUP_AUDIT_TABLES",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}