{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd787cc4-006c-46a3-96fc-a6ee21bdfcd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n╔═══════════════════════════════════════════════════════════════════════════╗\n║                                                                           ║\n║              \uD83C\uDFAF MASTER ORCHESTRATION NOTEBOOK                            ║\n║                                                                           ║\n║              Coordinating complete SEC Smart Money pipeline              ║\n║              Silver → Gold → Quality → Optimize                          ║\n║                                                                           ║\n╚═══════════════════════════════════════════════════════════════════════════╝\n\n\n\uD83D\uDCCB PIPELINE CONFIGURATION:\n  RUN_ID:            20260222_035752\n  EXECUTION_DATE:    2026-02-22\n  START_TIME:        03:57:52\n  Run Mode:          incremental\n  Environment:       production\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# MASTER ORCHESTRATION NOTEBOOK (FINAL - NO PARAMETERS!)\n",
    "# Main entry point for entire SEC Smart Money pipeline\n",
    "# Silver → Gold → Quality → Optimize\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "\n",
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════╗\n",
    "║                                                                           ║\n",
    "║              \uD83C\uDFAF MASTER ORCHESTRATION NOTEBOOK                            ║\n",
    "║                                                                           ║\n",
    "║              Coordinating complete SEC Smart Money pipeline              ║\n",
    "║              Silver → Gold → Quality → Optimize                          ║\n",
    "║                                                                           ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# STEP 1: Generate Execution IDs (No parameters!)\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXECUTION_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "START_TIME = datetime.now()\n",
    "\n",
    "run_mode = \"incremental\"\n",
    "environment = \"production\"\n",
    "\n",
    "print(f\"\"\"\n",
    "\uD83D\uDCCB PIPELINE CONFIGURATION:\n",
    "  RUN_ID:            {RUN_ID}\n",
    "  EXECUTION_DATE:    {EXECUTION_DATE}\n",
    "  START_TIME:        {START_TIME.strftime('%H:%M:%S')}\n",
    "  Run Mode:          {run_mode}\n",
    "  Environment:       {environment}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518cf760-249d-4cbc-953e-d60ad55f63af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDD0D Initializing audit logging...\n✅ Pipeline run logged: 20260222_035752\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# STEP 2: Initialize Audit Logging\n",
    "\n",
    "print(\"\\n\uD83D\uDD0D Initializing audit logging...\")\n",
    "\n",
    "def log_pipeline_start():\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO fintech_analytics.audit.pipeline_runs \n",
    "        (run_id, execution_date, environment, run_mode, status, start_time, total_tasks)\n",
    "        VALUES (\n",
    "            '{RUN_ID}',\n",
    "            '{EXECUTION_DATE}',\n",
    "            '{environment}',\n",
    "            '{run_mode}',\n",
    "            'RUNNING',\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            4\n",
    "        )\n",
    "        \"\"\")\n",
    "        print(f\"✅ Pipeline run logged: {RUN_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not log pipeline start (audit table may not exist): {str(e)[:100]}\")\n",
    "\n",
    "def log_task_start(task_name):\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO fintech_analytics.audit.task_runs\n",
    "        (run_id, task_name, status, start_time)\n",
    "        VALUES ('{RUN_ID}', '{task_name}', 'RUNNING', CURRENT_TIMESTAMP())\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def log_task_success(task_name, rows=0):\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE fintech_analytics.audit.task_runs\n",
    "        SET status = 'SUCCESS',\n",
    "            end_time = CURRENT_TIMESTAMP(),\n",
    "            rows_processed = {rows}\n",
    "        WHERE run_id = '{RUN_ID}' AND task_name = '{task_name}'\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def log_task_failure(task_name, error):\n",
    "    try:\n",
    "        error_msg = str(error)[:200].replace(\"'\", \"''\")\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE fintech_analytics.audit.task_runs\n",
    "        SET status = 'FAILED',\n",
    "            end_time = CURRENT_TIMESTAMP(),\n",
    "            error_message = '{error_msg}'\n",
    "        WHERE run_id = '{RUN_ID}' AND task_name = '{task_name}'\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "log_pipeline_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6123c04-1cb2-4e51-be5d-81c7b3b836ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nTASK 1: SILVER TRANSFORMATION\n================================================================================\n  ⚙️  Running silver transformation...\n    ✅ Silver transformation succeeded (394 rows)\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# STEP 3: TASK 1 - Silver Transformation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 1: SILVER TRANSFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "task_1_name = \"silver_transformation\"\n",
    "log_task_start(task_1_name)\n",
    "task_1_success = False\n",
    "\n",
    "try:\n",
    "    print(f\"  ⚙️  Running silver transformation...\")\n",
    "    \n",
    "    # Call your silver notebook - NO ARGUMENTS!\n",
    "    result = dbutils.notebook.run(\n",
    "        \"/Users/shreyashp042@gmail.com/fintech_pipeline/jobs/sec_smart_money_silver\",\n",
    "        timeout_seconds=1800,\n",
    "        arguments={}  # Empty - no parameters!\n",
    "    )\n",
    "    \n",
    "    # Count rows\n",
    "    silver_count = spark.sql(\n",
    "        \"SELECT COUNT(*) as count FROM fintech_analytics.silver.silver_fact_insider_transactions\"\n",
    "    ).collect()[0]['count']\n",
    "    \n",
    "    log_task_success(task_1_name, silver_count)\n",
    "    task_1_success = True\n",
    "    \n",
    "    print(f\"    ✅ Silver transformation succeeded ({silver_count:,} rows)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error = str(e)\n",
    "    log_task_failure(task_1_name, error)\n",
    "    task_1_success = False\n",
    "    print(f\"    ❌ Silver transformation failed: {error[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d5a487-d004-4d49-a478-d533703bded1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# STEP 4: Check Task 1 Status\n",
    "\n",
    "if not task_1_success:\n",
    "    print(\"\"\"\n",
    "❌ CRITICAL ERROR: Silver Transformation Failed\n",
    "   Pipeline cannot continue\n",
    "\n",
    "Actions:\n",
    "  1. Check silver notebook logs\n",
    "  2. Fix data source issue\n",
    "  3. Re-run pipeline\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE fintech_analytics.audit.pipeline_runs\n",
    "        SET status = 'FAILED', end_time = CURRENT_TIMESTAMP()\n",
    "        WHERE run_id = '{RUN_ID}'\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise Exception(\"Silver transformation failed - stopping pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236223b1-ae33-46d2-8e8c-485bbee8ff5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nTASK 2: GOLD ANALYTICS\n================================================================================\n  ⚙️  Running gold analytics...\n    ✅ Gold analytics succeeded (26 rows)\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# STEP 5: TASK 2 - Gold Analytics\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 2: GOLD ANALYTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "task_2_name = \"gold_analytics\"\n",
    "log_task_start(task_2_name)\n",
    "task_2_success = False\n",
    "\n",
    "try:\n",
    "    print(f\"  ⚙️  Running gold analytics...\")\n",
    "    \n",
    "    # Call your gold notebook - NO ARGUMENTS!\n",
    "    result = dbutils.notebook.run(\n",
    "        \"/Users/shreyashp042@gmail.com/fintech_pipeline/jobs/sec_smart_money_gold\",\n",
    "        timeout_seconds=1200,\n",
    "        arguments={}  # Empty - no parameters!\n",
    "    )\n",
    "    \n",
    "    # Count rows\n",
    "    gold_count = spark.sql(\n",
    "        \"SELECT COUNT(*) as count FROM fintech_analytics.gold.gold_insider_summary_by_company\"\n",
    "    ).collect()[0]['count']\n",
    "    \n",
    "    log_task_success(task_2_name, gold_count)\n",
    "    task_2_success = True\n",
    "    \n",
    "    print(f\"    ✅ Gold analytics succeeded ({gold_count:,} rows)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error = str(e)\n",
    "    log_task_failure(task_2_name, error)\n",
    "    task_2_success = False\n",
    "    print(f\"    ❌ Gold analytics failed: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "596b9883-0500-4439-ab60-4ec6c722a566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# STEP 6: Check Task 2 Status\n",
    "\n",
    "if not task_2_success:\n",
    "    print(\"\"\"\n",
    "❌ CRITICAL ERROR: Gold Analytics Failed\n",
    "   Pipeline cannot continue\n",
    "\n",
    "Actions:\n",
    "  1. Check gold notebook logs\n",
    "  2. Fix transformation logic\n",
    "  3. Re-run pipeline\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE fintech_analytics.audit.pipeline_runs\n",
    "        SET status = 'FAILED', end_time = CURRENT_TIMESTAMP()\n",
    "        WHERE run_id = '{RUN_ID}'\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise Exception(\"Gold analytics failed - stopping pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97dc3904-c25f-431a-a7d2-236859b5c06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nTASK 3: DATA QUALITY CHECKS\n================================================================================\n  ⚙️  Running quality checks...\n    ✅ Quality checks passed\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: TASK 3 - Data Quality Checks\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 3: DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "task_3_name = \"data_quality_checks\"\n",
    "log_task_start(task_3_name)\n",
    "task_3_success = False\n",
    "\n",
    "try:\n",
    "    print(f\"  ⚙️  Running quality checks...\")\n",
    "    \n",
    "    # Call quality checks - NO ARGUMENTS!\n",
    "    result = dbutils.notebook.run(\n",
    "        \"/Users/shreyashp042@gmail.com/fintech_pipeline/jobs/08_data_quality_checks\",\n",
    "        timeout_seconds=600,\n",
    "        arguments={}  # Empty - no parameters!\n",
    "    )\n",
    "    \n",
    "    log_task_success(task_3_name, 0)\n",
    "    task_3_success = True\n",
    "    \n",
    "    print(f\"    ✅ Quality checks passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error = str(e)\n",
    "    log_task_failure(task_3_name, error)\n",
    "    task_3_success = False\n",
    "    # Non-critical, so don't stop\n",
    "    print(f\"    ⚠️  Quality checks warning: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74b53ca-990c-44cb-8c7c-d7b77e5141c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nTASK 4: TABLE OPTIMIZATION\n================================================================================\n  ⚙️  Running table optimization...\n    ✅ Tables optimized\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: TASK 4 - Table Optimization (Non-Critical)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 4: TABLE OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "task_4_name = \"table_optimization\"\n",
    "log_task_start(task_4_name)\n",
    "task_4_success = False\n",
    "\n",
    "try:\n",
    "    print(f\"  ⚙️  Running table optimization...\")\n",
    "    \n",
    "    # Call optimization - NO ARGUMENTS!\n",
    "    result = dbutils.notebook.run(\n",
    "        \"/Users/shreyashp042@gmail.com/fintech_pipeline/jobs/table_optimization\",\n",
    "        timeout_seconds=900,\n",
    "        arguments={}  # Empty - no parameters!\n",
    "    )\n",
    "    \n",
    "    log_task_success(task_4_name, 0)\n",
    "    task_4_success = True\n",
    "    \n",
    "    print(f\"    ✅ Tables optimized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error = str(e)\n",
    "    log_task_failure(task_4_name, error)\n",
    "    task_4_success = False\n",
    "    # Non-critical, so don't stop\n",
    "    print(f\"    ⚠️  Optimization warning: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be47b359-fd9f-4e26-8184-78ebfdc69fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDCCA EXECUTION SUMMARY\n================================================================================\n  Run ID.................................. 20260222_035752\n  Status.................................. SUCCESS\n  Total Tasks............................. 4\n  Successful.............................. 4\n  Failed.................................. 0\n  Duration (sec).......................... 4043\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Calculate Summary\n",
    "\n",
    "total_tasks = 4\n",
    "successful_tasks = sum([task_1_success, task_2_success, task_3_success, task_4_success])\n",
    "failed_tasks = total_tasks - successful_tasks\n",
    "\n",
    "# Overall status based on critical tasks\n",
    "overall_status = \"SUCCESS\" if (task_1_success and task_2_success) else \"FAILED\"\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - START_TIME).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\uD83D\uDCCA EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = {\n",
    "    \"Run ID\": RUN_ID,\n",
    "    \"Status\": overall_status,\n",
    "    \"Total Tasks\": total_tasks,\n",
    "    \"Successful\": successful_tasks,\n",
    "    \"Failed\": failed_tasks,\n",
    "    \"Duration (sec)\": int(duration),\n",
    "}\n",
    "\n",
    "for key, value in summary_data.items():\n",
    "    print(f\"  {key:.<40} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b50552-ef30-44db-9d32-f49be5a097c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Pipeline status updated in audit table\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: Update Pipeline Status in Audit\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "    UPDATE fintech_analytics.audit.pipeline_runs\n",
    "    SET status = '{overall_status}',\n",
    "        end_time = CURRENT_TIMESTAMP(),\n",
    "        duration_seconds = {int(duration)},\n",
    "        successful_tasks = {successful_tasks},\n",
    "        failed_tasks = {failed_tasks}\n",
    "    WHERE run_id = '{RUN_ID}'\n",
    "    \"\"\")\n",
    "    print(f\"\\n✅ Pipeline status updated in audit table\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not update audit table: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed20280-5d35-4271-90d7-9020a5037b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDCCB AUDIT INFORMATION\n================================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_id</th><th>status</th><th>duration_seconds</th><th>successful_tasks</th><th>failed_tasks</th></tr></thead><tbody><tr><td>20260222_035752</td><td>SUCCESS</td><td>4043</td><td>4</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "20260222_035752",
         "SUCCESS",
         4043,
         4,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration_seconds",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "successful_tasks",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "failed_tasks",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 11: Display Audit Information\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\uD83D\uDCCB AUDIT INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    pipeline_info = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        run_id,\n",
    "        status,\n",
    "        duration_seconds,\n",
    "        successful_tasks,\n",
    "        failed_tasks\n",
    "    FROM fintech_analytics.audit.pipeline_runs\n",
    "    WHERE run_id = '{RUN_ID}'\n",
    "    \"\"\")\n",
    "    \n",
    "    display(pipeline_info)\n",
    "except Exception as e:\n",
    "    print(f\"Could not display audit info: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec401a40-bd6b-4296-a374-7555e3661e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCDD Task Execution Details:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>task_name</th><th>status</th><th>duration_sec</th></tr></thead><tbody><tr><td>silver_transformation</td><td>SUCCESS</td><td>null</td></tr><tr><td>silver_transformation</td><td>SUCCESS</td><td>null</td></tr><tr><td>silver_transformation</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>silver_transformation</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>gold_analytics</td><td>SUCCESS</td><td>null</td></tr><tr><td>data_quality_checks</td><td>SUCCESS</td><td>null</td></tr><tr><td>data_quality_checks</td><td>SUCCESS</td><td>null</td></tr><tr><td>data_quality_checks</td><td>SUCCESS</td><td>null</td></tr><tr><td>data_quality_checks</td><td>SUCCESS</td><td>null</td></tr><tr><td>table_optimization</td><td>SUCCESS</td><td>null</td></tr><tr><td>table_optimization</td><td>SUCCESS</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "silver_transformation",
         "SUCCESS",
         null
        ],
        [
         "silver_transformation",
         "SUCCESS",
         null
        ],
        [
         "silver_transformation",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "silver_transformation",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "gold_analytics",
         "SUCCESS",
         null
        ],
        [
         "data_quality_checks",
         "SUCCESS",
         null
        ],
        [
         "data_quality_checks",
         "SUCCESS",
         null
        ],
        [
         "data_quality_checks",
         "SUCCESS",
         null
        ],
        [
         "data_quality_checks",
         "SUCCESS",
         null
        ],
        [
         "table_optimization",
         "SUCCESS",
         null
        ],
        [
         "table_optimization",
         "SUCCESS",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "task_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration_sec",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# STEP 12: Show Task Details\n",
    "\n",
    "print(\"\\n\uD83D\uDCDD Task Execution Details:\")\n",
    "\n",
    "try:\n",
    "    task_details = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        task_name,\n",
    "        status,\n",
    "        ROUND(duration_seconds, 1) as duration_sec\n",
    "    FROM fintech_analytics.audit.task_runs\n",
    "    WHERE run_id = '{RUN_ID}'\n",
    "    ORDER BY start_time\n",
    "    \"\"\")\n",
    "    \n",
    "    display(task_details)\n",
    "except Exception as e:\n",
    "    print(f\"Could not display task details: {str(e)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a8cb2d-0e9f-486f-83f2-93f04e4218bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83C\uDF89 PIPELINE EXECUTION COMPLETE\n================================================================================\n\n✅ SUCCESS! Pipeline completed successfully\n\nSummary:\n  - Silver layer: Transformed ✅\n  - Gold layer: Aggregated ✅\n  - Quality checks: Validated ✅\n  - Tables optimized: ✅\n  - Duration: 4043 seconds\n  - Run ID: 20260222_035752\n\nYour data is ready for analysis!\n    \n"
     ]
    }
   ],
   "source": [
    "# STEP 13: Final Report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\uD83C\uDF89 PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if overall_status == \"SUCCESS\":\n",
    "    print(f\"\"\"\n",
    "✅ SUCCESS! Pipeline completed successfully\n",
    "\n",
    "Summary:\n",
    "  - Silver layer: Transformed ✅\n",
    "  - Gold layer: Aggregated ✅\n",
    "  - Quality checks: Validated {'✅' if task_3_success else '⚠️'}\n",
    "  - Tables optimized: {'✅' if task_4_success else '⚠️'}\n",
    "  - Duration: {int(duration)} seconds\n",
    "  - Run ID: {RUN_ID}\n",
    "\n",
    "Your data is ready for analysis!\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "⚠️ WARNING: Pipeline completed with issues\n",
    "\n",
    "Status: {overall_status}\n",
    "  - Silver layer: {'✅' if task_1_success else '❌'}\n",
    "  - Gold layer: {'✅' if task_2_success else '❌'}\n",
    "  - Quality checks: {'✅' if task_3_success else '⚠️'}\n",
    "  - Tables optimized: {'✅' if task_4_success else '⚠️'}\n",
    "\n",
    "Review error logs and fix issues.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10de05c0-4ac1-4e6f-918d-8baed1e45276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 14: Return Status (Safe)\n",
    "\n",
    "try:\n",
    "    result = {\n",
    "        \"status\": overall_status,\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"successful_tasks\": successful_tasks,\n",
    "        \"failed_tasks\": failed_tasks,\n",
    "        \"duration_seconds\": int(duration)\n",
    "    }\n",
    "    dbutils.jobs.taskValues.set(result)\n",
    "    print(f\"\\n✅ Results returned to workflow\")\n",
    "except:\n",
    "    # OK if not in a job\n",
    "    print(f\"\\n⚠️  Not running in a job context (OK for testing)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Final Status\n",
    "\n",
    "print(f\"\"\"\n",
    "═══════════════════════════════════════════════════════════════\n",
    "Pipeline Execution: {overall_status}\n",
    "Run ID: {RUN_ID}\n",
    "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "═══════════════════════════════════════════════════════════════\n",
    "\"\"\")\n",
    "\n",
    "if overall_status == \"SUCCESS\":\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "else:\n",
    "    dbutils.notebook.exit(\"FAILED\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "05_MASTER_ORCHESTRATION",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}